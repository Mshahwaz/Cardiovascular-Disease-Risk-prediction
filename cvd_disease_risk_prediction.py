# -*- coding: utf-8 -*-
"""CVD disease Risk Prediction.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/117VDoO-uAdeY13kqUJejoTphnWI0pHhM

### Importing the Dependencies
"""

import numpy as np
import pandas as pd
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split

"""###reading csv file from pandas dataframe"""

heart_data=pd.read_csv('/content/heart_statlog_cleveland_hungary_final2.csv')

"""### cheching no of rows and columns in dataset"""

heart_data.shape

heart_data.columns

"""### Getting some info about dataset"""

heart_data.info()

"""### cheching for missing values

"""

heart_data.isnull().sum()

"""###Analyse the distribution of data"""

import matplotlib.pyplot as plt
import seaborn as sns

fig, ax=plt.subplots(figsize=(8,8))
sns.distplot(heart_data. fasting_blood_sugar)

fig, ax=plt.subplots(figsize=(8,8))
sns.distplot(heart_data. chest_pain_type)

fig, ax=plt.subplots(figsize=(8,8))
sns.distplot(heart_data. resting_bp_s)

fig, ax=plt.subplots(figsize=(8,8))
sns.distplot(heart_data. resting_ecg)

fig, ax=plt.subplots(figsize=(8,8))
sns.distplot(heart_data. max_heart_rate)

fig, ax=plt.subplots(figsize=(8,8))
sns.distplot(heart_data. exercise_angina)

fig, ax=plt.subplots(figsize=(8,8))
sns.distplot(heart_data. oldpeak)

fig, ax=plt.subplots(figsize=(8,8))
sns.distplot(heart_data. ST_slope)

fig, ax=plt.subplots(figsize=(8,8))
sns.distplot(heart_data. target)

"""### Replacing the missing or outliers with mean value"""

heart_data['cholesterol']=heart_data['cholesterol'].replace(0,heart_data['cholesterol'].mean())

fig, ax=plt.subplots(figsize=(8,8))
sns.distplot(heart_data. cholesterol)

heart_data

heart_data.std()

heart_data.shape

fig, ax=plt.subplots(figsize=(8,8))
sns.distplot(heart_data. cholesterol)

X=heart_data.drop(columns='target', axis=1)
Y=heart_data['target']

print(X)

print(Y)

heart_data['target'].value_counts()

"""#**using train test split**"""

X_train, X_test, Y_train, Y_test=train_test_split(X, Y, test_size=0.2, random_state=3)

from sklearn.preprocessing import StandardScaler    
st_x= StandardScaler()    
X_train= st_x.fit_transform(X_train)    
X_test= st_x.transform(X_test)

print(X.shape, X_train.shape, X_test.shape)

print(X_train)

print(Y.shape, Y_train.shape, Y_test.shape)

from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score

model=LogisticRegression(max_iter=1000, random_state=0)

model.fit(X_train, Y_train)

X_train_prediction=model.predict(X_train)
training_data_accu=accuracy_score(X_train_prediction, Y_train)

print(training_data_accu)

X_test_prediction=model.predict(X_test)
training_data_accu=accuracy_score(X_test_prediction, Y_test)
print(training_data_accu)

#Fitting Decision Tree classifier to the training set  
from sklearn.ensemble import RandomForestClassifier

classifier= RandomForestClassifier(n_estimators= 10, criterion="entropy")

classifier.fit(X_train, Y_train)

X_train_prediction_ran=classifier.predict(X_train)
training_data_accu=accuracy_score(X_train_prediction_ran, Y_train)

print(training_data_accu)

X_test_prediction_ran=classifier.predict(X_test)
training_data_accu_ran=accuracy_score(X_test_prediction_ran, Y_test)

print(training_data_accu_ran)

from sklearn.svm import SVC # "Support vector classifier"  
classifier = SVC(kernel='linear', random_state=0)

classifier.fit(X_train, Y_train)

X_train_prediction_svc=classifier.predict(X_train)
training_data_accu_svc=accuracy_score(X_train_prediction_svc, Y_train)

print(training_data_accu_svc)

#Fitting Decision Tree classifier to the training set  
from sklearn.tree import DecisionTreeClassifier
classifier= DecisionTreeClassifier(criterion='gini', random_state=2)  
classifier.fit(X_train, Y_train)

X_train_prediction_svc=classifier.predict(X_train)
training_data_accu_svc=accuracy_score(X_train_prediction_svc, Y_train)

print(training_data_accu_svc)

models=[LogisticRegression(max_iter=1000, random_state=0),RandomForestClassifier(n_estimators= 10, criterion="entropy"),SVC(kernel='linear', random_state=0)]

"""#Comparing  invidually"""

def compare_model():
  for model in models:
    model.fit(X_train, Y_train)
    test_pred=model.predict(X_test)
    accu=accuracy_score(Y_test, test_pred)
    print("Accuracy of ",model,"= ",accu)

compare_model()

"""#comparing using cross validation"""

from sklearn.model_selection import cross_val_score

cv_score_lr=cross_val_score(LogisticRegression(max_iter=1000, random_state=0), X, Y, cv=5)

print(cv_score_lr)

mean=sum(cv_score_lr)/len(cv_score_lr)

mean=mean*100

mean=round(mean, 2)

print(mean)

cv_score_lr=cross_val_score(RandomForestClassifier(criterion='entropy', n_estimators=10), X, Y, cv=5)

print(cv_score_lr)

mean=sum(cv_score_lr)/len(cv_score_lr)

mean=mean*100

mean=round(mean, 2)

print(mean)

cv_score_lr=cross_val_score(SVC(kernel='linear', random_state=0), X, Y, cv=5)

print(cv_score_lr)

mean=sum(cv_score_lr)/len(cv_score_lr)

mean=mean*100

mean=round(mean, 2)

print(mean)

models=[LogisticRegression(max_iter=1000, random_state=0),RandomForestClassifier(n_estimators= 10, criterion="entropy"),SVC(kernel='linear', random_state=0)]
def compare_score_cv_score():
  for model in models:
    cv_score_compare=cross_val_score(model, X ,Y, cv=5)
    mean=sum(cv_score_compare)/len(cv_score_compare)

    mean=mean*100

    mean=round(mean, 2)

    print("Cross validation score for the ",model,"= ",cv_score_compare)
    print("Mean Accuracy of the ",model,"= ",mean)
    print("...................................................................")

compare_score_cv_score()

"""#prediction"""

input_data=(54,1,4,140,166,0,0,118,1,0,2)

input_numpy=np.asarray(input_data)

input_reshape=input_numpy.reshape(1,-1)


prediction_ran=classifier.predict(input_reshape)

print(prediction_ran)